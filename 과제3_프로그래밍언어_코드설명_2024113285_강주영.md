
### *1.기본적인  코드 설명은 주석으로 대체*
### *2.자세한 설명은 하단에 기록*
## 요약하는 함수 

``` python
def summaryText(text_list, char_limit=100):
    
    text = " ".join(text_list)
    
    # 파서 및 요약기 초기화
    parser = PlaintextParser.from_string(text, Tokenizer("korean")) 
    summarizer = LsaSummarizer()
    
    # 요약된 문장 추출
    sentence_count = len(parser.document.sentences)
    all_sentences = [str(sent) for sent in summarizer(parser.document, sentence_count)]
    
    # 글자 수 제한 내에서 문장 결합
    summary = ""
    for sentence in all_sentences:
        if len(summary + sentence) <= char_limit:
            summary += sentence + " "
        else:
            break
            
    return summary.strip()

```

1. def summaryText(text_list, char_limit=100):
	1. def : 함수 정의 시 사용
	2. text_list, char_limit : 함수의 매개변수, 함수 호출 시 char_limit을 정하지 않으면, 100으로 default vlaue(정수)설정
2. text = " ".join(text_list)
	1. text_list에 있는 문자열을 공백 기준으로 하나의 문자열로 결합하여, 요약 작업을 할 수 있도록 설정
3. PlaintextParser.from_string(text, Tokenizer("korean"))
	1. **PlaintextParser**: **sumy** 라이브러리에서 제공하는 클래스로 주어진 텍스트 요약할 수 있도록 파싱
	2. **Tokenizer("korean")**
		1. Tokenizer`는 텍스트를 단어 또는 토큰으로 분리하는 역할
		2. 여기서는 한국어를 처리하기 위해 "korean" 토크나이저를 사용
		3. 한국어 문법에 맞게 텍스트를 분해
		4. korean으로 설정하면, PC 환경에 맞춰 설치 할 것이 있음
	3. summarizer = LsaSummarizer()
		1. **(LSA)** 방식의 요약 기법을 사용하여 텍스트에서 중요한 문장을 추출
	4. sentence_count = len(parser.document.sentences)
		1. 요약할 문장의 개수를 결정
	5. all_sentences = [str(sent) for sent in summarizer(parser.document, sentence_count)]
		1. summarizer를 사용하여 선택된 문장 수 만큼 문장을 추출하여, 각 문장을 문자열로 변환하여 all_sentences리스트에 저장
	6. for sentence in all_sentences: ...
		1. 추출된 문장들을 하나씩 순찬적으로 summary문자열에 추가하여, 글자수가 char_limit(ex : 100)을 초과하지 않도록 설정
	7. return summary.strip()
		1. 최종 요약된 텍스트 반환
## 크롤링 함수 + 실행 함수 
``` python
def fetchDonggukNews():
    # 동국대학교 학사 공지 url
    url = "https://www.dongguk.edu/article/HAKSANOTICE/list?category_cd=&article_seq=&flag=&pageIndex=1&searchCondition=TA.SUBJECT&searchKeyword="
    search_string = "공지"

    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        originalText = []
        # 크롤링할 데이터 찾기 
        articles = soup.select('.board_list p.tit') 

        for i, article in enumerate(articles[:15]):  # 최대 15개의 기사 요약

            title = article.get_text(strip=True)
            originalText.append(title)

            if search_string in title:
              # 공지라는 단어가 있으면 '공지' 앞뒤에 [ ]를 추가
              title = title.replace(search_string, "* ["+search_string+"] ", 1)

            print(f"{i+1}. {title}\n")

        print("=========요약내용=========")
        #print(originalText)
        result = summaryText(originalText)
        print(result)
    except Exception as e:
        print(f"오류 발생: {e}")

# 매일 8시 작업 예약
schedule.every().day.at("08:00").do(fetchDonggukNews)
#fetchDonggukNews()


while True:
   schedule.run_pending()
    time.sleep(1)

```

1. def fetchDonggukNews(): 
	1. 동국대학교 학사 공지사항 페이지에서 공지사항 제목을 크롤링하기 위해 만든 함수
	2. url 변수에도 동국대학교 학사 공지사항 주소추가
2. try-catch 
	1. 예외처리, 오류가 생긴다면 catch 문으로 빠진 후 오류발생 + 메세지 출력하도록 설정
3. response = requests.get(url)
	response.raise_for_status()
	soup = BeautifulSoup(response.text, 'html.parser')
	1. 웹 요청 및 응답처리
	2. request 라이브러리를 사용하여 지정된 URL에서 데이터를 가져오도록 설정
	3. response.raise_for_status() : HTTP 요청이 성공적이지 않으면 오류를 발생시키도록 설정
	4. BeautifulSoup 을 사용하여 페이지의 HTML 을 파싱하여 soup객체로 저장
4. articles = soup.select('.board_list p.tit') 
	1. 동국대학교 학사 공지사항에서 제목 부분만 가져올 수 있도록 설정
	2. board_list 클래스를 가진 태그 안의 p태그 의 tit클래스
5. for i, article in enumerate(articles[:15])...
	1. title = article.get_text(strip=True)
		1. articles에 저장된 제목에서 최대 15개만 가져오도록 반환
	2. originalText.append(title) 
		1. 추출한 제목들 요약하해주는 summaryText() 함수의 매개변수 text_list로 보내기 위한 작업
	3. if search_string in title:
		title = title.replace(search_string, "* ["+search_string+"] ", 1)
		1. 만약 공지라는 단어가 있으면 '공지' 앞에 '*[' 를 뒤에 ']' 를 추가
	4. print(f"{i+1}. {title}\n")
		1. f-string 을 사용하여 문자열 포매팅 후 출력
		2. 크롤링한 가사 제됨
	5. result = summaryText(originalText)
        print(result)
		1. 추출한 제목을 smmaryText() 함수에 전달하여 반환된 요약한 값을 result로 받은 후 해당 값 출력
6. schedule.every().day.at("08:00").do(fetchDonggukNews)
	1. 매일 오전 8시에 `fetchDonggukNews()` 함수가 실행되도록 예약
	2. python라이브러리 사용
7. while True:
	schedule.run_pending();
	 time.sleep(1)
	 1. 무한루프를 돌면서 예약된 작업(schedule.run_pending();)을 수행하고, (time.sleep(1))1초마다 기다림. 이로써 매일 정해진 시간에 자동으로 작업하도록 설정